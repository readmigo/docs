# Readmigo ä¹¦ç±å¯¼å…¥ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

> Version: 1.2.0
> Status: Implementation Complete (Core + Batch Management)
> Author: System Architect
> Last Updated: 2025-12-27

---

## 1. æ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡

æ„å»ºä¸€ä¸ª**å¯æ‰©å±•ã€å¤šæºã€è‡ªåŠ¨åŒ–**çš„ä¹¦ç±å¯¼å…¥ç³»ç»Ÿï¼Œæ”¯æ’‘ Readmigo çš„å†…å®¹è¿è¥éœ€æ±‚ï¼š

- **å¤šæºæ”¯æŒ**: æ”¯æŒ Standard Ebooksã€Project Gutenbergã€ä¸­æ–‡å¤ç±ç­‰å¤šä¸ªå…¬ç‰ˆä¹¦æº
- **å¤šç¯å¢ƒéš”ç¦»**: æœ¬åœ°ã€Stagingã€Production ç¯å¢ƒç‹¬ç«‹é…ç½®ï¼Œæ•°æ®éš”ç¦»
- **æ‰¹é‡å¯¼å…¥**: æ”¯æŒæµ·é‡ä¹¦ç±çš„æ‰¹é‡å¯¼å…¥ï¼Œå¸¦è¿›åº¦è¿½è¸ªå’Œæ–­ç‚¹ç»­ä¼ 
- **è‡ªåŠ¨å¤„ç†**: EPUB è§£æã€å°é¢æå–ã€éš¾åº¦åˆ†æã€åˆ†ç±»æ ‡æ³¨è‡ªåŠ¨åŒ–
- **è´¨é‡æ§åˆ¶**: å¯¼å…¥å‰é¢„æ£€ã€å»é‡æœºåˆ¶ã€é”™è¯¯å¤„ç†å’Œå›æ»š

### 1.2 æ ¸å¿ƒåŸåˆ™

| åŸåˆ™ | è¯´æ˜ |
|------|------|
| **å¹‚ç­‰æ€§** | é‡å¤å¯¼å…¥åŒä¸€ä¹¦ç±ä¸ä¼šäº§ç”Ÿé‡å¤æ•°æ® |
| **å¯è¿½æº¯** | æ¯æœ¬ä¹¦è®°å½•æ¥æºã€å¯¼å…¥æ—¶é—´ã€å¯¼å…¥æ‰¹æ¬¡ |
| **å¯å›æ»š** | æ”¯æŒæŒ‰æ‰¹æ¬¡å›æ»šå¯¼å…¥çš„ä¹¦ç± |
| **æ¸è¿›å¼** | æ”¯æŒå¢é‡å¯¼å…¥ï¼Œåªå¯¼å…¥æ–°ä¹¦ç± |
| **ç¯å¢ƒéš”ç¦»** | ä¸åŒç¯å¢ƒä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“å’Œå­˜å‚¨ |

### 1.3 åœ¨ç³»ç»Ÿä¸­çš„å®šä½

```mermaid
flowchart TD
    subgraph Sources["External Book Sources"]
        SE["Standard Ebooks<br>~1,000æœ¬"]
        PG["Project Gutenberg<br>~70,000æœ¬"]
        CT["CTEXT<br>(å¤å…¸æ–‡çŒ®)"]
        WS["Wikisource<br>(ZH)"]
    end

    SE --> BIS
    PG --> BIS
    CT --> BIS
    WS --> BIS

    subgraph BIS["Book Import System"]
        SF["Source Fetcher"] --> EP["EPUB Parser"] --> DA["Difficulty Analyzer"] --> CP["Cover Processor"]
        SF --> IPM
        EP --> IPM
        DA --> IPM
        CP --> IPM
        IPM["Import Pipeline Manager<br>- ä»»åŠ¡è°ƒåº¦ - è¿›åº¦è¿½è¸ª<br>- é”™è¯¯å¤„ç† - æ‰¹æ¬¡ç®¡ç†"]
    end

    IPM --> DB["PostgreSQL<br>(Books)"]
    IPM --> R2["Cloudflare R2<br>(Files)"]
    IPM --> RD["Redis<br>(Queue)"]
```

---

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æ•´ä½“æ¶æ„

```mermaid
flowchart TD
    subgraph CLI["CLI / Script Layer"]
        SH["book-import.sh<br>--env=local|staging|production<br>--source=standard|gutenberg|chinese|all<br>--limit=N / --dry-run"]
    end

    SH --> SAL

    subgraph SAL["Source Adapters Layer"]
        SEA["Standard Ebooks<br>Adapter"]
        GA["Gutenberg<br>Adapter"]
        CSA["Chinese Sources<br>Adapter"]
        BSI["Interface: BookSourceAdapter<br>fetchBookList / fetchBookContent / fetchCover"]
        SEA --> BSI
        GA --> BSI
        CSA --> BSI
    end

    BSI --> PL

    subgraph PL["Processing Layer"]
        subgraph IP["Import Pipeline"]
            GM["1. Get Meta"] --> EP2["2. EPUB Parse"] --> DS["3. Diff Score"] --> R2S["4. R2 Store"]
            GM --> DC["Dedup Check"]
            EP2 --> CH["Clean HTML"]
            DS --> CL["CEFR Level"]
            R2S --> TG["Thumb Gen"]
        end
        EPP["EPUB Parser<br>Metadata / Chapters<br>Content / ToC"]
        DAN["Difficulty Analyzer<br>Flesch / Syllables<br>CEFR Map"]
        CVP["Cover Processor<br>Download / Resize<br>Compress"]
    end

    PL --> SL

    subgraph SL["Storage Layer"]
        PGS["PostgreSQL<br>Book / Chapter<br>Author / ImportBatch"]
        CFR["Cloudflare R2<br>EPUB Files<br>Cover Images / Thumbnails"]
        RED["Redis<br>Job Queue / Progress<br>Dedup Cache"]
    end
```

### 2.2 å¤šç¯å¢ƒæ¶æ„

```mermaid
flowchart LR
    subgraph ENV["Multi-Environment Architecture"]
        direction LR
        LOCAL["LOCAL<br>DB: localhost:5432<br>R2: readmigo-dev<br><br>ç”¨é€”:<br>æœ¬åœ°å¼€å‘ / åŠŸèƒ½æµ‹è¯• / å¿«é€Ÿè¿­ä»£<br>æƒé™: å¼€å‘è€…<br>ç¡®è®¤: æ— "]
        STAGING["STAGING<br>DB: staging-db.readmigo.com:5432<br>R2: readmigo-staging<br><br>ç”¨é€”:<br>é¢„å‘å¸ƒæµ‹è¯• / æ•°æ®éªŒè¯ / æ€§èƒ½æµ‹è¯•<br>æƒé™: è¿è¥+å¼€å‘<br>ç¡®è®¤: å•æ¬¡"]
        PRODUCTION["PRODUCTION<br>DB: prod-db.readmigo.com:5432<br>R2: readmigo-prod<br><br>ç”¨é€”:<br>çœŸå®ç”¨æˆ· / æ­£å¼æ•°æ® / éœ€å®¡æ‰¹<br>æƒé™: ä»…ç®¡ç†å‘˜<br>ç¡®è®¤: äºŒæ¬¡"]
    end

    LOCAL -.->|éš”ç¦»| STAGING -.->|éš”ç¦»| PRODUCTION
```

---

## 3. æ•°æ®æºè®¾è®¡

### 3.1 æ”¯æŒçš„ä¹¦ç±æ¥æº

| æ¥æº | ç±»å‹ | æ•°é‡ | è¯­è¨€ | ç‰¹ç‚¹ |
|------|------|------|------|------|
| Standard Ebooks | å…¬ç‰ˆä¹¦ | ~1,000 | è‹±è¯­ | é«˜è´¨é‡æ’ç‰ˆã€ç²¾æ ¡æ–‡æœ¬ |
| Project Gutenberg | å…¬ç‰ˆä¹¦ | ~70,000 | å¤šè¯­è¨€ | æ•°é‡æœ€å¤§ã€è¦†ç›–å¹¿ |
| Gutenberg-ZH | å…¬ç‰ˆä¹¦ | ~500 | ä¸­æ–‡ | ä¸­æ–‡ç»å…¸æ–‡å­¦ |
| CTEXT | å¤ç± | ~10,000 | å¤æ–‡ | ä¸­å›½å¤å…¸æ–‡çŒ® |
| Wikisource-ZH | ç»´åŸºæ–‡åº“ | ~5,000 | ä¸­æ–‡ | ç°ä»£ä¸­æ–‡ä½œå“ |

### 3.2 æ•°æ®æºé€‚é…å™¨æ¥å£

```typescript
// types/book-source.ts

export interface BookSourceAdapter {
  /** æ•°æ®æºå”¯ä¸€æ ‡è¯† */
  readonly sourceId: BookSource;

  /** æ•°æ®æºåç§° */
  readonly sourceName: string;

  /** æ”¯æŒçš„è¯­è¨€ */
  readonly supportedLanguages: string[];

  /**
   * è·å–ä¹¦ç±åˆ—è¡¨ (å…ƒæ•°æ®)
   * @param options åˆ†é¡µå’Œç­›é€‰é€‰é¡¹
   * @returns ä¹¦ç±å…ƒæ•°æ®åˆ—è¡¨
   */
  fetchBookList(options?: FetchOptions): AsyncGenerator<BookMetadata>;

  /**
   * è·å–å•æœ¬ä¹¦ç±çš„ EPUB å†…å®¹
   * @param sourceBookId æ•°æ®æºä¸­çš„ä¹¦ç±ID
   * @returns EPUB æ–‡ä»¶ Buffer
   */
  fetchBookContent(sourceBookId: string): Promise<Buffer>;

  /**
   * è·å–ä¹¦ç±å°é¢
   * @param sourceBookId æ•°æ®æºä¸­çš„ä¹¦ç±ID
   * @returns å°é¢å›¾ç‰‡ Buffer
   */
  fetchCover(sourceBookId: string): Promise<Buffer | null>;

  /**
   * æ£€æŸ¥ä¹¦ç±æ˜¯å¦å·²å­˜åœ¨ (å»é‡)
   * @param metadata ä¹¦ç±å…ƒæ•°æ®
   * @returns æ˜¯å¦å·²å¯¼å…¥
   */
  checkExists(metadata: BookMetadata): Promise<boolean>;
}

export interface BookMetadata {
  sourceId: BookSource;
  sourceBookId: string;
  sourceUrl: string;

  title: string;
  author: string;
  authorBirthYear?: number;
  authorDeathYear?: number;

  language: string;
  languageVariant?: string;  // zh-Hans, zh-Hant, en-US, en-GB

  description?: string;
  subjects: string[];
  genres: string[];

  publishedYear?: number;
  wordCount?: number;
  pageCount?: number;

  coverUrl?: string;
  epubUrl: string;

  license?: string;
  rights?: string;
}

export interface FetchOptions {
  limit?: number;
  offset?: number;
  language?: string;
  afterDate?: Date;  // å¢é‡å¯¼å…¥: åªè·å–æ­¤æ—¥æœŸä¹‹åæ›´æ–°çš„
}

export enum BookSource {
  STANDARD_EBOOKS = 'STANDARD_EBOOKS',
  GUTENBERG = 'GUTENBERG',
  GUTENBERG_ZH = 'GUTENBERG_ZH',
  CTEXT = 'CTEXT',
  WIKISOURCE_ZH = 'WIKISOURCE_ZH',
  USER_UPLOAD = 'USER_UPLOAD',
}
```

### 3.3 Standard Ebooks é€‚é…å™¨å®ç°

```typescript
// sources/standard-ebooks.ts

import { BookSourceAdapter, BookMetadata, BookSource, FetchOptions } from '../types';

export class StandardEbooksAdapter implements BookSourceAdapter {
  readonly sourceId = BookSource.STANDARD_EBOOKS;
  readonly sourceName = 'Standard Ebooks';
  readonly supportedLanguages = ['en'];

  private readonly baseUrl = 'https://standardebooks.org';
  private readonly opdsUrl = 'https://standardebooks.org/opds';

  async *fetchBookList(options?: FetchOptions): AsyncGenerator<BookMetadata> {
    // ä½¿ç”¨ OPDS feed è·å–ä¹¦ç±åˆ—è¡¨
    const response = await fetch(`${this.opdsUrl}/all`);
    const xml = await response.text();
    const entries = this.parseOPDS(xml);

    let count = 0;
    for (const entry of entries) {
      if (options?.limit && count >= options.limit) break;

      const metadata = this.entryToMetadata(entry);

      // å¢é‡æ£€æŸ¥
      if (options?.afterDate && metadata.publishedYear) {
        const bookDate = new Date(metadata.publishedYear, 0, 1);
        if (bookDate < options.afterDate) continue;
      }

      yield metadata;
      count++;
    }
  }

  async fetchBookContent(sourceBookId: string): Promise<Buffer> {
    // Standard Ebooks çš„ URL æ ¼å¼: /ebooks/{author}/{title}
    const epubUrl = `${this.baseUrl}/ebooks/${sourceBookId}/downloads/${this.getEpubFilename(sourceBookId)}`;

    const response = await fetch(epubUrl);
    if (!response.ok) {
      throw new Error(`Failed to fetch EPUB: ${response.status}`);
    }

    return Buffer.from(await response.arrayBuffer());
  }

  async fetchCover(sourceBookId: string): Promise<Buffer | null> {
    const coverUrl = `${this.baseUrl}/ebooks/${sourceBookId}/cover.jpg`;

    try {
      const response = await fetch(coverUrl);
      if (!response.ok) return null;
      return Buffer.from(await response.arrayBuffer());
    } catch {
      return null;
    }
  }

  async checkExists(metadata: BookMetadata): Promise<boolean> {
    // é€šè¿‡ sourceId + sourceBookId æŸ¥è¯¢æ•°æ®åº“
    const existing = await prisma.book.findFirst({
      where: {
        source: this.sourceId,
        sourceId: metadata.sourceBookId,
      },
    });
    return !!existing;
  }

  private parseOPDS(xml: string): OPDSEntry[] {
    // è§£æ OPDS XML...
    // è¿”å›ä¹¦ç±æ¡ç›®åˆ—è¡¨
  }

  private entryToMetadata(entry: OPDSEntry): BookMetadata {
    return {
      sourceId: this.sourceId,
      sourceBookId: entry.id,
      sourceUrl: entry.link,
      title: entry.title,
      author: entry.author,
      language: 'en',
      description: entry.summary,
      subjects: entry.subjects || [],
      genres: this.mapSubjectsToGenres(entry.subjects),
      epubUrl: entry.epubLink,
      coverUrl: entry.coverLink,
    };
  }

  private getEpubFilename(sourceBookId: string): string {
    // ç”Ÿæˆ EPUB æ–‡ä»¶å
    const slug = sourceBookId.replace(/\//g, '_');
    return `${slug}.epub`;
  }

  private mapSubjectsToGenres(subjects: string[]): string[] {
    // å°† Standard Ebooks çš„ subjects æ˜ å°„åˆ°å†…éƒ¨åˆ†ç±»
    const genreMap: Record<string, string> = {
      'Fiction': 'fiction',
      'Science Fiction': 'sci-fi',
      'Mystery': 'mystery',
      // ...
    };

    return subjects
      .map(s => genreMap[s])
      .filter(Boolean);
  }
}
```

### 3.4 Project Gutenberg é€‚é…å™¨

```typescript
// sources/gutenberg.ts

export class GutenbergAdapter implements BookSourceAdapter {
  readonly sourceId = BookSource.GUTENBERG;
  readonly sourceName = 'Project Gutenberg';
  readonly supportedLanguages = ['en', 'de', 'fr', 'es', 'it', 'pt', 'zh'];

  private readonly catalogUrl = 'https://www.gutenberg.org/cache/epub/feeds/rdf-files.tar.bz2';
  private readonly mirrorUrl = 'https://www.gutenberg.org/ebooks';

  async *fetchBookList(options?: FetchOptions): AsyncGenerator<BookMetadata> {
    // Gutenberg ä½¿ç”¨ RDF ç›®å½•
    // 1. ä¸‹è½½ RDF ç›®å½•å‹ç¼©åŒ…
    // 2. è§£å‹å¹¶è§£ææ¯æœ¬ä¹¦çš„ RDF æ–‡ä»¶
    // 3. è½¬æ¢ä¸º BookMetadata

    const catalog = await this.loadCatalog();

    let count = 0;
    for (const rdf of catalog) {
      if (options?.limit && count >= options.limit) break;

      // è¯­è¨€ç­›é€‰
      if (options?.language && rdf.language !== options.language) continue;

      // åªå¯¼å…¥æœ‰ EPUB æ ¼å¼çš„ä¹¦ç±
      if (!rdf.hasEpub) continue;

      yield this.rdfToMetadata(rdf);
      count++;
    }
  }

  async fetchBookContent(sourceBookId: string): Promise<Buffer> {
    // Gutenberg EPUB URL æ ¼å¼: /ebooks/{id}.epub.images
    const epubUrl = `${this.mirrorUrl}/${sourceBookId}.epub.images`;

    const response = await fetch(epubUrl);
    if (!response.ok) {
      // å°è¯•æ— å›¾ç‰‡ç‰ˆæœ¬
      const fallbackUrl = `${this.mirrorUrl}/${sourceBookId}.epub.noimages`;
      const fallbackResponse = await fetch(fallbackUrl);
      if (!fallbackResponse.ok) {
        throw new Error(`Failed to fetch EPUB: ${response.status}`);
      }
      return Buffer.from(await fallbackResponse.arrayBuffer());
    }

    return Buffer.from(await response.arrayBuffer());
  }

  async fetchCover(sourceBookId: string): Promise<Buffer | null> {
    const coverUrl = `${this.mirrorUrl}/${sourceBookId}/cover.medium.jpg`;

    try {
      const response = await fetch(coverUrl);
      if (!response.ok) return null;
      return Buffer.from(await response.arrayBuffer());
    } catch {
      return null;
    }
  }
}
```

---

## 4. å¯¼å…¥æµæ°´çº¿è®¾è®¡

### 4.1 å¯¼å…¥æµç¨‹

```mermaid
flowchart TD
    subgraph P1["1. FETCH PHASE (è·å–é˜¶æ®µ)"]
        SA["Source Adapter"] --> FM["Fetch Meta"] --> DDC["Dedup Check"] --> QB["Queue Book"]
        DDC --> SKIP["Skip if exists"]
    end

    P1 --> P2

    subgraph P2["2. DOWNLOAD PHASE (ä¸‹è½½é˜¶æ®µ)"]
        DE["Download EPUB"] --> DC["Download Cover"] --> TS["Temp Storage"]
        DE --> RETRY["Retry on failure"]
        DC --> PLACEHOLDER["Generate placeholder<br>if missing"]
    end

    P2 --> P3

    subgraph P3["3. PARSE PHASE (è§£æé˜¶æ®µ)"]
        PE["Parse EPUB"] --> EC["Extract Chapters"] --> CHTML["Clean HTML"] --> ET["Extract ToC"]
        PE --> META["Metadata validation"]
        EC --> CHEXT["Chapter[] extraction"]
        CHTML --> CLEAN["Clean Text content"]
        ET --> NAV["Navigation structure"]
    end

    P3 --> P4

    subgraph P4["4. ANALYZE PHASE (åˆ†æé˜¶æ®µ)"]
        CW["Count Words"] --> FS["Flesch Score"] --> CEFR["CEFR Level"] --> AC["Auto Category"]
        CW --> WC["wordCount / charCount"]
        FS --> DIFF["difficultyScore (0-100)"]
        CEFR --> LEVEL["cefrLevel (A1-C2)"]
        AC --> CATS["categories[]"]
    end

    P4 --> P5

    subgraph P5["5. UPLOAD PHASE (ä¸Šä¼ é˜¶æ®µ)"]
        UE["Upload EPUB"] --> UC["Upload Cover"] --> GT["Generate Thumb"]
        UE --> R2B["R2: /books/{id}.epub"]
        UC --> R2C["R2: /covers/{id}.jpg"]
        GT --> R2T["R2: /thumbs/{id}.jpg"]
    end

    P5 --> P6

    subgraph P6["6. SAVE PHASE (ä¿å­˜é˜¶æ®µ)"]
        CB["Create Book"] --> CC["Create Chapters"] --> LC["Link Category"] --> LI["Log Import"]
        CB --> PGB["PostgreSQL Book table"]
        CC --> PGC["PostgreSQL Chapter tbl"]
        LC --> BCJ["BookCategory join table"]
        LI --> IML["ImportLog"]
    end
```

### 4.2 å¯¼å…¥ä»»åŠ¡ç®¡ç†

```typescript
// pipeline/import-manager.ts

export interface ImportBatch {
  id: string;
  source: BookSource;
  environment: Environment;

  status: ImportStatus;
  startedAt: Date;
  completedAt?: Date;

  totalBooks: number;
  processedBooks: number;
  successBooks: number;
  failedBooks: number;
  skippedBooks: number;  // å·²å­˜åœ¨çš„

  errors: ImportError[];

  createdBy?: string;
}

export interface ImportProgress {
  batchId: string;
  current: number;
  total: number;
  currentBook?: string;
  phase: ImportPhase;
  eta?: number;  // é¢„è®¡å‰©ä½™ç§’æ•°
}

export enum ImportStatus {
  PENDING = 'PENDING',
  RUNNING = 'RUNNING',
  PAUSED = 'PAUSED',
  COMPLETED = 'COMPLETED',
  FAILED = 'FAILED',
  CANCELLED = 'CANCELLED',
}

export enum ImportPhase {
  FETCHING = 'FETCHING',
  DOWNLOADING = 'DOWNLOADING',
  PARSING = 'PARSING',
  ANALYZING = 'ANALYZING',
  UPLOADING = 'UPLOADING',
  SAVING = 'SAVING',
}

export class ImportManager {
  constructor(
    private prisma: PrismaClient,
    private redis: Redis,
    private storage: StorageService,
  ) {}

  /**
   * å¼€å§‹å¯¼å…¥æ‰¹æ¬¡
   */
  async startBatch(options: StartBatchOptions): Promise<ImportBatch> {
    const batch: ImportBatch = {
      id: ulid(),
      source: options.source,
      environment: options.environment,
      status: ImportStatus.RUNNING,
      startedAt: new Date(),
      totalBooks: 0,
      processedBooks: 0,
      successBooks: 0,
      failedBooks: 0,
      skippedBooks: 0,
      errors: [],
    };

    // ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯
    await this.saveBatch(batch);

    // è·å–æ•°æ®æºé€‚é…å™¨
    const adapter = this.getAdapter(options.source);

    // å¼€å§‹å¯¼å…¥
    try {
      for await (const metadata of adapter.fetchBookList(options)) {
        batch.totalBooks++;

        // æ›´æ–°è¿›åº¦
        await this.updateProgress(batch.id, {
          current: batch.processedBooks,
          total: batch.totalBooks,
          currentBook: metadata.title,
          phase: ImportPhase.FETCHING,
        });

        try {
          // æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
          if (await adapter.checkExists(metadata)) {
            batch.skippedBooks++;
            batch.processedBooks++;
            continue;
          }

          // æ‰§è¡Œå¯¼å…¥æµæ°´çº¿
          await this.importBook(metadata, adapter, batch);
          batch.successBooks++;

        } catch (error) {
          batch.failedBooks++;
          batch.errors.push({
            bookId: metadata.sourceBookId,
            title: metadata.title,
            error: error.message,
            phase: this.currentPhase,
          });

          // æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢
          if (batch.failedBooks > options.maxErrors) {
            throw new Error('Too many errors, stopping import');
          }
        }

        batch.processedBooks++;
        await this.saveBatch(batch);

        // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
        if (options.limit && batch.processedBooks >= options.limit) {
          break;
        }
      }

      batch.status = ImportStatus.COMPLETED;
      batch.completedAt = new Date();

    } catch (error) {
      batch.status = ImportStatus.FAILED;
      batch.errors.push({
        error: error.message,
        phase: 'BATCH',
      });
    }

    await this.saveBatch(batch);
    return batch;
  }

  /**
   * å¯¼å…¥å•æœ¬ä¹¦ç±
   */
  private async importBook(
    metadata: BookMetadata,
    adapter: BookSourceAdapter,
    batch: ImportBatch,
  ): Promise<void> {
    // 1. ä¸‹è½½ EPUB
    this.currentPhase = ImportPhase.DOWNLOADING;
    const epubBuffer = await adapter.fetchBookContent(metadata.sourceBookId);
    const coverBuffer = await adapter.fetchCover(metadata.sourceBookId);

    // 2. è§£æ EPUB
    this.currentPhase = ImportPhase.PARSING;
    const parsed = await this.epubParser.parse(epubBuffer);

    // 3. åˆ†æéš¾åº¦
    this.currentPhase = ImportPhase.ANALYZING;
    const analysis = await this.difficultyAnalyzer.analyze(parsed.content, metadata.language);

    // 4. ä¸Šä¼ æ–‡ä»¶
    this.currentPhase = ImportPhase.UPLOADING;
    const bookId = ulid();
    const epubUrl = await this.storage.upload(`books/${bookId}.epub`, epubBuffer);
    const coverUrl = coverBuffer
      ? await this.uploadCover(bookId, coverBuffer)
      : null;

    // 5. ä¿å­˜åˆ°æ•°æ®åº“
    this.currentPhase = ImportPhase.SAVING;
    await this.prisma.$transaction(async (tx) => {
      // åˆ›å»ºä¹¦ç±
      const book = await tx.book.create({
        data: {
          id: bookId,
          title: metadata.title,
          author: metadata.author,
          description: metadata.description,
          language: metadata.language,
          languageVariant: metadata.languageVariant,

          source: metadata.sourceId,
          sourceId: metadata.sourceBookId,
          sourceUrl: metadata.sourceUrl,

          epubUrl,
          coverUrl,
          coverThumbUrl: coverUrl ? this.getThumbUrl(coverUrl) : null,

          wordCount: analysis.wordCount,
          characterCount: analysis.characterCount,
          difficultyScore: analysis.difficultyScore,
          fleschScore: analysis.fleschScore,
          cefrLevel: analysis.cefrLevel,

          subjects: metadata.subjects,
          genres: metadata.genres,

          status: 'PENDING',  // å¾…å®¡æ ¸
          importBatchId: batch.id,
        },
      });

      // åˆ›å»ºç« èŠ‚
      for (const chapter of parsed.chapters) {
        await tx.chapter.create({
          data: {
            bookId: book.id,
            order: chapter.order,
            title: chapter.title,
            href: chapter.href,
            content: chapter.content,
            wordCount: chapter.wordCount,
          },
        });
      }

      // å…³è”åˆ†ç±»
      if (metadata.genres.length > 0) {
        await this.linkCategories(tx, book.id, metadata.genres);
      }
    });
  }

  /**
   * ä¸Šä¼ å°é¢å¹¶ç”Ÿæˆç¼©ç•¥å›¾
   */
  private async uploadCover(bookId: string, coverBuffer: Buffer): Promise<string> {
    // å¤„ç†å°é¢å›¾ç‰‡
    const processed = await sharp(coverBuffer)
      .resize(400, 600, { fit: 'cover' })
      .jpeg({ quality: 85 })
      .toBuffer();

    // ç”Ÿæˆç¼©ç•¥å›¾
    const thumb = await sharp(coverBuffer)
      .resize(120, 180, { fit: 'cover' })
      .jpeg({ quality: 75 })
      .toBuffer();

    // ä¸Šä¼ 
    const coverUrl = await this.storage.upload(`covers/${bookId}.jpg`, processed);
    await this.storage.upload(`thumbs/${bookId}.jpg`, thumb);

    return coverUrl;
  }

  /**
   * è·å–å¯¼å…¥è¿›åº¦
   */
  async getProgress(batchId: string): Promise<ImportProgress | null> {
    const data = await this.redis.hgetall(`import:progress:${batchId}`);
    if (!data) return null;

    return {
      batchId,
      current: parseInt(data.current || '0'),
      total: parseInt(data.total || '0'),
      currentBook: data.currentBook,
      phase: data.phase as ImportPhase,
      eta: data.eta ? parseInt(data.eta) : undefined,
    };
  }

  /**
   * æš‚åœå¯¼å…¥
   */
  async pauseBatch(batchId: string): Promise<void> {
    // è®¾ç½®æš‚åœæ ‡å¿—
    await this.redis.set(`import:pause:${batchId}`, '1');
  }

  /**
   * å›æ»šæ‰¹æ¬¡
   */
  async rollbackBatch(batchId: string): Promise<void> {
    const batch = await this.getBatch(batchId);
    if (!batch) throw new Error('Batch not found');

    // è·å–è¯¥æ‰¹æ¬¡å¯¼å…¥çš„ä¹¦ç±
    const books = await this.prisma.book.findMany({
      where: { importBatchId: batchId },
      select: { id: true, epubUrl: true, coverUrl: true },
    });

    // åˆ é™¤æ–‡ä»¶
    for (const book of books) {
      await this.storage.delete(book.epubUrl);
      if (book.coverUrl) {
        await this.storage.delete(book.coverUrl);
        await this.storage.delete(this.getThumbUrl(book.coverUrl));
      }
    }

    // åˆ é™¤æ•°æ®åº“è®°å½•
    await this.prisma.$transaction([
      this.prisma.chapter.deleteMany({ where: { book: { importBatchId: batchId } } }),
      this.prisma.bookCategory.deleteMany({ where: { book: { importBatchId: batchId } } }),
      this.prisma.book.deleteMany({ where: { importBatchId: batchId } }),
    ]);

    // æ›´æ–°æ‰¹æ¬¡çŠ¶æ€
    await this.updateBatchStatus(batchId, ImportStatus.CANCELLED);
  }
}
```

---

## 5. éš¾åº¦åˆ†æç³»ç»Ÿ

### 5.1 è‹±æ–‡éš¾åº¦åˆ†æ

```typescript
// processors/difficulty-analyzer.ts

export interface DifficultyAnalysis {
  wordCount: number;
  characterCount: number;
  sentenceCount: number;
  syllableCount: number;

  averageWordLength: number;
  averageSentenceLength: number;
  averageSyllablesPerWord: number;

  fleschReadingEase: number;    // 0-100, è¶Šé«˜è¶Šæ˜“è¯»
  fleschKincaidGrade: number;   // ç¾å›½å¹´çº§æ°´å¹³
  difficultyScore: number;       // 0-100, æ ‡å‡†åŒ–éš¾åº¦åˆ†æ•°
  cefrLevel: CEFRLevel;          // A1-C2
}

export class EnglishDifficultyAnalyzer {
  /**
   * åˆ†ææ–‡æœ¬éš¾åº¦
   */
  analyze(text: string): DifficultyAnalysis {
    const words = this.tokenizeWords(text);
    const sentences = this.tokenizeSentences(text);
    const syllables = words.map(w => this.countSyllables(w));

    const wordCount = words.length;
    const sentenceCount = sentences.length;
    const syllableCount = syllables.reduce((a, b) => a + b, 0);
    const characterCount = text.replace(/\s/g, '').length;

    const avgWordLength = characterCount / wordCount;
    const avgSentenceLength = wordCount / sentenceCount;
    const avgSyllablesPerWord = syllableCount / wordCount;

    // Flesch Reading Ease
    const fleschReadingEase = 206.835
      - (1.015 * avgSentenceLength)
      - (84.6 * avgSyllablesPerWord);

    // Flesch-Kincaid Grade Level
    const fleschKincaidGrade = (0.39 * avgSentenceLength)
      + (11.8 * avgSyllablesPerWord)
      - 15.59;

    // æ ‡å‡†åŒ–éš¾åº¦åˆ†æ•° (0-100)
    const difficultyScore = this.normalizeDifficulty(fleschReadingEase);

    // æ˜ å°„åˆ° CEFR ç­‰çº§
    const cefrLevel = this.mapToCEFR(fleschReadingEase, avgWordLength);

    return {
      wordCount,
      characterCount,
      sentenceCount,
      syllableCount,
      averageWordLength: avgWordLength,
      averageSentenceLength: avgSentenceLength,
      averageSyllablesPerWord: avgSyllablesPerWord,
      fleschReadingEase,
      fleschKincaidGrade,
      difficultyScore,
      cefrLevel,
    };
  }

  /**
   * è®¡ç®—éŸ³èŠ‚æ•°
   */
  private countSyllables(word: string): number {
    word = word.toLowerCase().replace(/[^a-z]/g, '');
    if (word.length <= 3) return 1;

    // åŸºäºå…ƒéŸ³è®¡æ•°çš„ç®€åŒ–ç®—æ³•
    const vowels = word.match(/[aeiouy]+/g) || [];
    let count = vowels.length;

    // è°ƒæ•´è§„åˆ™
    if (word.endsWith('e') && !word.endsWith('le')) count--;
    if (word.endsWith('es') || word.endsWith('ed')) count--;
    if (count === 0) count = 1;

    return count;
  }

  /**
   * æ ‡å‡†åŒ–éš¾åº¦åˆ†æ•°
   */
  private normalizeDifficulty(fleschScore: number): number {
    // Flesch 0-30 (å¾ˆéš¾) -> éš¾åº¦ 70-100
    // Flesch 30-50 (éš¾) -> éš¾åº¦ 50-70
    // Flesch 50-60 (ä¸­ç­‰åéš¾) -> éš¾åº¦ 40-50
    // Flesch 60-70 (æ ‡å‡†) -> éš¾åº¦ 30-40
    // Flesch 70-80 (è¾ƒæ˜“) -> éš¾åº¦ 20-30
    // Flesch 80-90 (æ˜“) -> éš¾åº¦ 10-20
    // Flesch 90-100 (å¾ˆæ˜“) -> éš¾åº¦ 0-10

    const clamped = Math.max(0, Math.min(100, fleschScore));
    return Math.round(100 - clamped);
  }

  /**
   * æ˜ å°„åˆ° CEFR ç­‰çº§
   */
  private mapToCEFR(fleschScore: number, avgWordLength: number): CEFRLevel {
    // ç»¼åˆè€ƒè™‘å¯è¯»æ€§å’Œè¯æ±‡å¤æ‚åº¦
    if (fleschScore >= 90) return 'A1';
    if (fleschScore >= 80) return 'A2';
    if (fleschScore >= 70) return 'B1';
    if (fleschScore >= 60) return 'B2';
    if (fleschScore >= 50) return 'C1';
    return 'C2';
  }
}

export type CEFRLevel = 'A1' | 'A2' | 'B1' | 'B2' | 'C1' | 'C2';
```

### 5.2 ä¸­æ–‡éš¾åº¦åˆ†æ

```typescript
// processors/chinese-difficulty-analyzer.ts

export class ChineseDifficultyAnalyzer {
  private jieba: NodeJieba;
  private hskVocab: Map<string, number>;  // è¯ -> HSKç­‰çº§ (1-6)

  constructor() {
    this.jieba = require('nodejieba');
    this.hskVocab = this.loadHSKVocabulary();
  }

  analyze(text: string): ChineseDifficultyAnalysis {
    // åˆ†è¯
    const words = this.jieba.cut(text);
    const characters = text.replace(/\s/g, '').split('');

    // ç»Ÿè®¡
    const wordCount = words.length;
    const characterCount = characters.length;
    const uniqueCharacters = new Set(characters).size;

    // HSK ç­‰çº§åˆ†æ
    const hskDistribution = this.analyzeHSKDistribution(words);
    const averageHSK = this.calculateAverageHSK(hskDistribution);

    // å¥å­å¤æ‚åº¦
    const sentences = this.splitSentences(text);
    const avgSentenceLength = characterCount / sentences.length;

    // éš¾åº¦åˆ†æ•°
    const difficultyScore = this.calculateDifficulty(
      averageHSK,
      avgSentenceLength,
      uniqueCharacters / characterCount,
    );

    // HSK ç­‰çº§
    const hskLevel = this.mapToHSK(difficultyScore);

    return {
      wordCount,
      characterCount,
      uniqueCharacters,
      sentenceCount: sentences.length,
      averageSentenceLength: avgSentenceLength,
      hskDistribution,
      averageHSK,
      difficultyScore,
      hskLevel,
    };
  }

  private analyzeHSKDistribution(words: string[]): Record<number, number> {
    const distribution: Record<number, number> = { 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0 };

    for (const word of words) {
      const level = this.hskVocab.get(word) || 7;  // 7 = è¶…çº²
      distribution[level]++;
    }

    return distribution;
  }

  private calculateAverageHSK(distribution: Record<number, number>): number {
    let total = 0;
    let count = 0;

    for (const [level, num] of Object.entries(distribution)) {
      total += parseInt(level) * num;
      count += num;
    }

    return count > 0 ? total / count : 1;
  }

  private mapToHSK(difficultyScore: number): number {
    if (difficultyScore <= 15) return 1;
    if (difficultyScore <= 30) return 2;
    if (difficultyScore <= 45) return 3;
    if (difficultyScore <= 60) return 4;
    if (difficultyScore <= 75) return 5;
    if (difficultyScore <= 90) return 6;
    return 7;  // è¶…çº²/å¤æ–‡
  }
}
```

---

## 6. CLI è„šæœ¬è®¾è®¡

### 6.1 ä¸»è„šæœ¬ç»“æ„

```bash
scripts/
â”œâ”€â”€ book-import.sh           # ä¸»å…¥å£è„šæœ¬ (Shell)
â”œâ”€â”€ book-ingestion/          # TypeScript å¯¼å…¥ä»£ç 
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ .env                 # é»˜è®¤é…ç½®
â”‚   â”œâ”€â”€ .env.local           # æœ¬åœ°ç¯å¢ƒ
â”‚   â”œâ”€â”€ .env.staging         # Staging ç¯å¢ƒ
â”‚   â”œâ”€â”€ .env.production      # ç”Ÿäº§ç¯å¢ƒ
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline.ts          # å¯¼å…¥æµæ°´çº¿å…¥å£
â”‚   â”œâ”€â”€ pipeline-chinese.ts  # ä¸­æ–‡ä¹¦ç±å¯¼å…¥
â”‚   â”‚
â”‚   â”œâ”€â”€ sources/             # æ•°æ®æºé€‚é…å™¨
â”‚   â”‚   â”œâ”€â”€ standard-ebooks.ts
â”‚   â”‚   â”œâ”€â”€ gutenberg.ts
â”‚   â”‚   â”œâ”€â”€ gutenberg-zh.ts
â”‚   â”‚   â”œâ”€â”€ ctext.ts
â”‚   â”‚   â””â”€â”€ wikisource-zh.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ processors/          # å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ epub-parser.ts
â”‚   â”‚   â”œâ”€â”€ html-cleaner.ts
â”‚   â”‚   â”œâ”€â”€ difficulty-analyzer.ts
â”‚   â”‚   â”œâ”€â”€ chinese-difficulty-analyzer.ts
â”‚   â”‚   â””â”€â”€ cover-processor.ts
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ storage.ts
â”‚       â”œâ”€â”€ logger.ts
â”‚       â””â”€â”€ progress.ts
```

### 6.2 ç¯å¢ƒé…ç½®æ–‡ä»¶

```env
# .env.local

# æ•°æ®åº“
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/readmigo?schema=public

# Redis
REDIS_URL=redis://localhost:6379

# Cloudflare R2
R2_ACCOUNT_ID=xxx
R2_ACCESS_KEY_ID=xxx
R2_SECRET_ACCESS_KEY=xxx
R2_BUCKET_NAME=readmigo-dev
R2_PUBLIC_URL=https://cdn-dev.readmigo.app

# å¯¼å…¥é…ç½®
IMPORT_BATCH_SIZE=100
IMPORT_CONCURRENCY=5
IMPORT_RETRY_COUNT=3
IMPORT_TIMEOUT=30000

# æ—¥å¿—
LOG_LEVEL=debug
LOG_FILE=.logs/book-import.log
```

```env
# .env.production (æ•æ„Ÿä¿¡æ¯é€šè¿‡ CI/CD secrets æ³¨å…¥)

# æ•°æ®åº“
DATABASE_URL=${PROD_DATABASE_URL}

# Redis
REDIS_URL=${PROD_REDIS_URL}

# Cloudflare R2
R2_ACCOUNT_ID=${R2_ACCOUNT_ID}
R2_ACCESS_KEY_ID=${R2_ACCESS_KEY_ID}
R2_SECRET_ACCESS_KEY=${R2_SECRET_ACCESS_KEY}
R2_BUCKET_NAME=readmigo-prod
R2_PUBLIC_URL=https://cdn.readmigo.app

# å¯¼å…¥é…ç½® (ç”Ÿäº§ç¯å¢ƒæ›´ä¿å®ˆ)
IMPORT_BATCH_SIZE=50
IMPORT_CONCURRENCY=3
IMPORT_RETRY_COUNT=5
IMPORT_TIMEOUT=60000

# æ—¥å¿—
LOG_LEVEL=info
```

### 6.3 ä½¿ç”¨ç¤ºä¾‹

```bash
# 1. æœ¬åœ°ç¯å¢ƒ - å¯¼å…¥ Standard Ebooks å‰ 100 æœ¬
./scripts/book-import.sh --standard --limit=100

# 2. æœ¬åœ°ç¯å¢ƒ - å¯¼å…¥æ‰€æœ‰ Gutenberg è‹±æ–‡ä¹¦
./scripts/book-import.sh --gutenberg

# 3. Staging ç¯å¢ƒ - é¢„è§ˆå¯¼å…¥ (ä¸å®é™…å†™å…¥)
./scripts/book-import.sh --env=staging --all --dry-run

# 4. Staging ç¯å¢ƒ - å¯¼å…¥ä¸­æ–‡ä¹¦ç±
./scripts/book-import.sh --env=staging --chinese

# 5. ç”Ÿäº§ç¯å¢ƒ - å¯¼å…¥ (éœ€äºŒæ¬¡ç¡®è®¤)
./scripts/book-import.sh --env=production --standard

# 6. æŸ¥çœ‹å½“å‰ç»Ÿè®¡
./scripts/book-import.sh --stats

# 7. æŸ¥çœ‹å¯¼å…¥å†å²
./scripts/book-import.sh --history

# 8. å›æ»šæœ€è¿‘ä¸€æ¬¡å¯¼å…¥
./scripts/book-import.sh --rollback --batch-id=xxx
```

---

## 7. æ•°æ®æ¨¡å‹

### 7.1 Prisma Schema æ‰©å±•

```prisma
// å¯¼å…¥æ‰¹æ¬¡è®°å½•
model ImportBatch {
  id              String        @id @default(uuid())

  source          BookSource
  environment     String        @default("local")

  status          ImportStatus  @default(PENDING)
  startedAt       DateTime      @default(now())
  completedAt     DateTime?

  totalBooks      Int           @default(0)
  processedBooks  Int           @default(0)
  successBooks    Int           @default(0)
  failedBooks     Int           @default(0)
  skippedBooks    Int           @default(0)

  errors          Json?         // ImportError[]

  createdBy       String?
  notes           String?

  books           Book[]

  @@index([source, status])
  @@index([startedAt])
  @@map("import_batches")
}

enum ImportStatus {
  PENDING
  RUNNING
  PAUSED
  COMPLETED
  FAILED
  CANCELLED
}

// Book è¡¨æ‰©å±•å­—æ®µ
model Book {
  // ... ç°æœ‰å­—æ®µ

  // å¯¼å…¥æ¥æº
  source          BookSource?
  sourceId        String?       @map("source_id")
  sourceUrl       String?       @map("source_url")

  // å¯¼å…¥æ‰¹æ¬¡
  importBatchId   String?       @map("import_batch_id")
  importBatch     ImportBatch?  @relation(fields: [importBatchId], references: [id])
  importedAt      DateTime?     @map("imported_at")

  // å»é‡ç´¢å¼•
  @@unique([source, sourceId])
  @@index([importBatchId])
}

enum BookSource {
  STANDARD_EBOOKS
  GUTENBERG
  GUTENBERG_ZH
  CTEXT
  WIKISOURCE_ZH
  USER_UPLOAD
}
```

---

## 8. ç›‘æ§ä¸å‘Šè­¦

### 8.1 å¯¼å…¥ç›‘æ§æŒ‡æ ‡

```typescript
// ç›‘æ§æŒ‡æ ‡
interface ImportMetrics {
  // æ€§èƒ½æŒ‡æ ‡
  booksPerMinute: number;
  averageProcessTime: number;  // ms per book
  downloadSpeed: number;       // bytes/s
  uploadSpeed: number;         // bytes/s

  // è´¨é‡æŒ‡æ ‡
  successRate: number;         // æˆåŠŸç‡
  duplicateRate: number;       // é‡å¤ç‡
  errorRate: number;           // é”™è¯¯ç‡

  // èµ„æºæŒ‡æ ‡
  memoryUsage: number;
  cpuUsage: number;
  diskUsage: number;
}

// å‘Šè­¦è§„åˆ™
const alertRules = [
  {
    name: 'high_error_rate',
    condition: 'errorRate > 10%',
    severity: 'warning',
    action: 'pause_import',
  },
  {
    name: 'very_high_error_rate',
    condition: 'errorRate > 25%',
    severity: 'critical',
    action: 'stop_import_and_notify',
  },
  {
    name: 'slow_import',
    condition: 'booksPerMinute < 5',
    severity: 'warning',
    action: 'notify_ops',
  },
];
```

### 8.2 æ—¥å¿—æ ¼å¼

```typescript
// å¯¼å…¥æ—¥å¿—æ ¼å¼
interface ImportLogEntry {
  timestamp: string;
  level: 'debug' | 'info' | 'warn' | 'error';
  batchId: string;
  phase: ImportPhase;
  bookId?: string;
  bookTitle?: string;
  message: string;
  details?: any;
  duration?: number;
}

// æ—¥å¿—ç¤ºä¾‹
// 2025-01-15 10:30:00 [INFO] [batch-123] [DOWNLOADING] Downloading "Pride and Prejudice"...
// 2025-01-15 10:30:05 [INFO] [batch-123] [PARSING] Parsing EPUB (245KB)...
// 2025-01-15 10:30:08 [INFO] [batch-123] [ANALYZING] Difficulty: 45, CEFR: B1
// 2025-01-15 10:30:10 [INFO] [batch-123] [SAVING] Saved book-456 with 32 chapters
```

---

## 9. å®‰å…¨è€ƒè™‘

### 9.1 ç¯å¢ƒéš”ç¦»

```mermaid
flowchart LR
    LOCAL["LOCAL<br>å¼€å‘è€…æœºå™¨<br>æ— é™åˆ¶"] -->|éš”ç¦»| STAGING["STAGING<br>å†…éƒ¨æœåŠ¡å™¨<br>VPNè®¿é—® / æ—¥å¿—è®°å½•"]
    STAGING -->|éš”ç¦»| PROD["PRODUCTION<br>äº‘æœåŠ¡å™¨<br>éœ€å®¡æ‰¹ / MFAéªŒè¯ / å®¡è®¡æ—¥å¿—"]

    PROD --> MEASURES["é˜²æŠ¤æªæ–½:<br>- ç¯å¢ƒå˜é‡ä¸å«æ•æ„Ÿä¿¡æ¯ (secrets æ³¨å…¥)<br>- ç”Ÿäº§ç¯å¢ƒéœ€è¦ 2FA å’Œæ˜ç¡®æˆæƒ<br>- æ‰€æœ‰å¯¼å…¥æ“ä½œè®°å½•åˆ°å®¡è®¡æ—¥å¿—<br>- æ”¯æŒæ‰¹æ¬¡å›æ»š<br>- è‡ªåŠ¨å¤‡ä»½å¯¼å…¥å‰çš„æ•°æ®åº“å¿«ç…§"]
```

### 9.2 è®¿é—®æ§åˆ¶

```typescript
// ç¯å¢ƒè®¿é—®æ§åˆ¶
const environmentAccess: Record<Environment, AccessControl> = {
  local: {
    requiredRole: 'developer',
    requiresApproval: false,
    requiresMFA: false,
    maxBatchSize: Infinity,
  },
  staging: {
    requiredRole: 'developer',
    requiresApproval: false,
    requiresMFA: false,
    maxBatchSize: 10000,
  },
  production: {
    requiredRole: 'admin',
    requiresApproval: true,
    requiresMFA: true,
    maxBatchSize: 1000,
    requiresBackup: true,
  },
};
```

---

## 10. å®æ–½è®¡åˆ’

### Phase 1: åŸºç¡€æ¡†æ¶ âœ… COMPLETED

- [x] CLI è„šæœ¬æ¡†æ¶æ­å»º (`pipeline.ts`, `pipeline-chinese.ts`)
- [x] ç¯å¢ƒé…ç½®ç®¡ç† (`.env` æ–‡ä»¶)
- [x] æ•°æ®åº“ Schema æ‰©å±• (Book, Chapter, Author æ¨¡å‹)
- [x] æ—¥å¿—ç³»ç»Ÿ (console æ—¥å¿—)

### Phase 2: æ ¸å¿ƒå¯¼å…¥ âœ… COMPLETED

- [x] Standard Ebooks é€‚é…å™¨ (`sources/standard-ebooks.ts`)
- [x] EPUB è§£æå™¨ (`processors/epub-parser.ts`)
- [x] å°é¢å¤„ç†å™¨ (integrated in adapter)
- [x] R2 å­˜å‚¨é›†æˆ (EPUB, covers, thumbnails)

### Phase 3: éš¾åº¦åˆ†æ âœ… COMPLETED

- [x] è‹±æ–‡éš¾åº¦åˆ†æå™¨ (`processors/difficulty-analyzer.ts`)
- [x] Flesch Reading Ease & Flesch-Kincaid Grade
- [x] 1-10 éš¾åº¦ç­‰çº§æ˜ å°„
- [x] ä¸­æ–‡éš¾åº¦åˆ†æå™¨ (`processors/chinese-difficulty-analyzer.ts`)

### Phase 4: æ‰©å±•æ•°æ®æº âœ… MOSTLY COMPLETED

- [x] Project Gutenberg é€‚é…å™¨ (`sources/gutenberg.ts`)
- [x] Gutenberg-ZH ä¸­æ–‡é€‚é…å™¨ (`sources/gutenberg-zh.ts`)
- [x] LibriVox æœ‰å£°ä¹¦é€‚é…å™¨ (`sources/librivox.ts`)
- [ ] CTEXT å¤ç±æ•°æ®æº (æ¡†æ¶å·²å»ºç«‹)
- [ ] Wikisource-ZH æ•°æ®æº (æ¡†æ¶å·²å»ºç«‹)

### Phase 5: è¿ç»´åŠŸèƒ½ ğŸš§ IN PROGRESS

- [ ] æ‰¹æ¬¡ç®¡ç†å’Œå›æ»šåŠŸèƒ½ (è®¾è®¡å®Œæˆï¼Œå¾…å®ç°)
- [ ] ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
- [ ] Dashboard é›†æˆ

---

## 11. å¾…ç¡®è®¤äº‹é¡¹

1. **å¯¼å…¥å®¡æ ¸æµç¨‹**: å¯¼å…¥çš„ä¹¦ç±æ˜¯å¦éœ€è¦äººå·¥å®¡æ ¸æ‰èƒ½ä¸Šæ¶ï¼Ÿ
2. **å°é¢ç¼ºå¤±å¤„ç†**: æ— å°é¢ä¹¦ç±æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆå ä½å›¾ï¼Ÿ
3. **åˆ†ç±»æ˜ å°„è§„åˆ™**: å„æ•°æ®æºçš„åˆ†ç±»å¦‚ä½•ç»Ÿä¸€æ˜ å°„ï¼Ÿ
4. **å¢é‡å¯¼å…¥é¢‘ç‡**: æ˜¯å¦éœ€è¦å®šæ—¶ä»»åŠ¡è‡ªåŠ¨æ‹‰å–æ–°ä¹¦ï¼Ÿ
5. **å­˜å‚¨æˆæœ¬**: R2 å­˜å‚¨çš„é¢„ç®—å’Œæ¸…ç†ç­–ç•¥ï¼Ÿ

---

## å®æ–½è¿›åº¦

| ç‰ˆæœ¬ | çŠ¶æ€ | å®Œæˆåº¦ | æ›´æ–°æ—¥æœŸ | è¯´æ˜ |
|------|------|--------|----------|------|
| v1.3 | âœ… è¿ç»´å®Œæˆ | **95%** | 2025-12-28 | æ ¸å¿ƒå¯¼å…¥+æ‰¹æ¬¡ç®¡ç†+ç›‘æ§å‘Šè­¦+Dashboardå®Œæˆ |

### å·²å®Œæˆ âœ…

**æ¶æ„ä¸è®¾è®¡**
- [x] ç³»ç»Ÿæ•´ä½“æ¶æ„è®¾è®¡
- [x] å¤šç¯å¢ƒéš”ç¦»æ¶æ„è®¾è®¡ï¼ˆLocal/Staging/Productionï¼‰
- [x] æ•°æ®æºé€‚é…å™¨æ¥å£è®¾è®¡
- [x] å¯¼å…¥æµæ°´çº¿6ä¸ªé˜¶æ®µè®¾è®¡
- [x] Prisma Schema (Book, Chapter, Author æ¨¡å‹)

**æ ¸å¿ƒå¯¼å…¥åŠŸèƒ½**
- [x] CLI è„šæœ¬ä¸»å…¥å£ (`pipeline.ts`, `pipeline-chinese.ts`)
- [x] EPUB è§£æå™¨ (`processors/epub-parser.ts`) - JSDOM + AdmZip
- [x] HTML æ¸…ç†å™¨ (`processors/html-cleaner.ts`)
- [x] Cloudflare R2 å­˜å‚¨é›†æˆ (EPUB, covers, thumbnails)

**éš¾åº¦åˆ†æ**
- [x] è‹±æ–‡éš¾åº¦åˆ†æå™¨ (`processors/difficulty-analyzer.ts`)
  - Flesch Reading Ease
  - Flesch-Kincaid Grade Level
  - 1-10 éš¾åº¦ç­‰çº§æ˜ å°„
- [x] ä¸­æ–‡éš¾åº¦åˆ†æå™¨ (`processors/chinese-difficulty-analyzer.ts`)
  - HSK è¯æ±‡ç­‰çº§åˆ†æ
  - å¥å­å¤æ‚åº¦åˆ†æ

**æ•°æ®æºé€‚é…å™¨**
- [x] Standard Ebooks (`sources/standard-ebooks.ts`) - Web scraping å®ç°
- [x] Project Gutenberg (`sources/gutenberg.ts`) - å®Œæ•´å®ç°
- [x] Gutenberg-ZH (`sources/gutenberg-zh.ts`) - ä¸­æ–‡ä¹¦ç±
- [x] LibriVox (`sources/librivox.ts`) - æœ‰å£°ä¹¦æº

**è¾…åŠ©è„šæœ¬**
- [x] å°é¢æ›´æ–°è„šæœ¬ (`standard-ebooks-update-covers.ts`, `gutenberg-update-covers.ts`)
- [x] ä½œè€…å¯¼å…¥ (`import-authors.ts`, `import-top100-authors.ts`)
- [x] ä½œè€…å»é‡ (`deduplicate-authors.ts`)
- [x] æœ‰å£°ä¹¦ R2 åŒæ­¥ (`sync-audiobook-r2.ts`)
- [x] EPUB ä¸‹è½½ä¿®å¤è„šæœ¬ (`download-missing-epubs.ts`, `fix-single-book.ts`)

**ä¸­æ–‡æ•°æ®æºé€‚é…å™¨** (å·²å®Œæˆ)
- [x] CTEXT å¤ç±æ•°æ®æº (`sources/ctext.ts`) - CText API é›†æˆï¼Œå« 12 éƒ¨æ ¸å¿ƒå¤ç±
- [x] Wikisource-ZH æ•°æ®æº (`sources/wikisource-zh.ts`) - MediaWiki API é›†æˆï¼Œå«å››å¤§åè‘—ç­‰

**æ‰¹æ¬¡ç®¡ç†åŠŸèƒ½** (å·²å®Œæˆ)
- [x] ImportBatch æ‰¹æ¬¡ç®¡ç†æ•°æ®åº“æ¨¡å‹ (Prisma schema)
- [x] æ‰¹æ¬¡å¯¼å…¥å†å²æŸ¥è¯¢ API (`GET /admin/import/batches`)
- [x] æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯ API (`GET /admin/import/batches/stats`)
- [x] æ‰¹æ¬¡å›æ»šåŠŸèƒ½ (`DELETE /admin/import/batches/:id/rollback`)
- [x] æ‰¹æ¬¡çŠ¶æ€ç®¡ç† (start/complete/cancel)

**ç›‘æ§å‘Šè­¦åŠŸèƒ½** (å·²å®Œæˆ)
- [x] å¯¼å…¥ç›‘æ§æœåŠ¡ (`import-monitoring.service.ts`)
- [x] å¯¼å…¥æŒ‡æ ‡ API (`GET /admin/import/monitoring/metrics`)
- [x] å‘Šè­¦è§„åˆ™å¼•æ“ (`GET /admin/import/monitoring/alerts`)
- [x] å¥åº·çŠ¶æ€ API (`GET /admin/import/monitoring/health`)
- [x] æ´»åŠ¨æ‘˜è¦ API (`GET /admin/import/monitoring/activity`)
- [x] Dashboard ç›‘æ§é¢æ¿ (Import Batches > Monitoring tab)

### å¾…å¼€å‘ ğŸ“

**è¿ç»´åŠŸèƒ½**
- [x] å¯¼å…¥ç›‘æ§å’Œå‘Šè­¦ (Metrics, Alerts, Health Status API)
- [x] Dashboard ä¹¦ç±ç®¡ç†ç•Œé¢é›†æˆ (Import Batch Management + Monitoring UI)

**å¢å¼ºåŠŸèƒ½**
- [x] å¢é‡å¯¼å…¥ API (`GET /admin/import/incremental`, `GET /admin/import/should-import`)
- [ ] å¹¶å‘æ§åˆ¶ (IMPORT_CONCURRENCY)
- [ ] æ–­ç‚¹ç»­ä¼ 
- [ ] CEFR ç­‰çº§è‡ªåŠ¨æ˜ å°„
- [ ] è‡ªåŠ¨åˆ†ç±»æ ‡æ³¨

### ä¾èµ–é¡¹çŠ¶æ€

| ä¾èµ– | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| Cloudflare R2 | âœ… | å·²é…ç½®ï¼Œç”¨äºå­˜å‚¨ EPUB å’Œå°é¢ |
| PostgreSQL | âœ… | å·²éƒ¨ç½²ï¼Œå­˜å‚¨ä¹¦ç±å…ƒæ•°æ® |
| Prisma ORM | âœ… | å·²é…ç½®ï¼Œæ•°æ®åº“è®¿é—®å±‚ |
| Redis | ğŸ“ | å¯é€‰ï¼Œç”¨äºä»»åŠ¡é˜Ÿåˆ— |
| Dashboard | ğŸ“ | å¾…å¼€å‘ï¼Œä¹¦ç±ç®¡ç†ç•Œé¢ |

### æŠ€æœ¯å€ºåŠ¡

| é¡¹ç›® | ä¼˜å…ˆçº§ | è¯´æ˜ |
|------|--------|------|
| å¢é‡å¯¼å…¥ | ä¸­ | æ”¯æŒåªå¯¼å…¥æ–°ä¹¦ç± |
| å¹¶å‘æ§åˆ¶ | ä½ | æ§åˆ¶åŒæ—¶å¤„ç†çš„ä¹¦ç±æ•°é‡ |
| æ–­ç‚¹ç»­ä¼  | ä½ | å¯¼å…¥ä¸­æ–­åæ¢å¤ |
| å¯¼å…¥æµ‹è¯• | ä¸­ | æ·»åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯• |
